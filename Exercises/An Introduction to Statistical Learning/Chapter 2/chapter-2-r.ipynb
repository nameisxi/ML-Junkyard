{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Exercises on R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Flexible method would work better since in-flexible method wouldn't be able to predict anything else than linear values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In-flexible method like linear regression would work better since low number of datapoints would cause overfitting on a more flexible method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Flexible method would work better because of the non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) If the variance is already high, then a more flexible model would not work any better. In-flexible model could be a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) This is a regression problem since the thing of interest is a number. It seems like inference is what we're after here instead of prediction accuracy because of the interest towards correlations between variables. Here n = 500 and p = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) This is clearly a classification problem since the prediction will either be \"success or failure\". We're also looking for accurate predictions becuase there are no mentions of need for better understanding of possible correlations. Here n = 20 and p = 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) There is a clear numerical prediction goal, so this is a regression problem. This also is a prediction problem because we want accurate model without requirement for easy understanding. Here n = 52 and p = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) 3a.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Bias becomes lower as flexibility increases because flexible models assumptions towards shape of data is lower. Variance becomes higher because more flexible models are prone to overfit and usually fit more \"closer\" to training data points. Training error approaches zero as flexibility increases because of the aforementioned overfitting as flexibility increases. Test error first becomes smaller, but as it gets closer to the Bayes error, its slope becomes smaller and smaller. At some point test error starts to increase because of the overfitting happening in the model. Bayes error is a constant since it gives the best possible prediction for every type of data. It's not constant at zero because it also represents to irreducible error in any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) 1) Fraud detection where variables contain information about transactions. Prediction would result either to one or zero.\n",
    "2) Specific disease detection where variables contain specific biological values like blood pressure. Prediction would again be either one or zero. 3) Loan payment prediction where variables include financial history of a customer and prediction would be a representation of customers likelyhood of paying back to the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is highly non-linear, it would result to much better results. It should be noted that the downsides include higher change of overfitting to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance <- function(a1, a2, a3, p) {\n",
    "    return(sqrt((a1 - p)^2 + (a2 - p)^2 + (a3 - p)^2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = c(0, 2, 0, 0, -1, 1)\n",
    "X2 = c(3, 0, 1, 1, 0, 1)\n",
    "X3 = c(0, 0, 3, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 3\n",
      "[1] 2\n",
      "[1] 3.162278\n",
      "[1] 2.236068\n",
      "[1] 1.414214\n",
      "[1] 1.732051\n"
     ]
    }
   ],
   "source": [
    "r = length(X1)\n",
    "for (i in 1:r) {\n",
    "    print(euclidean_distance(X1[i], X2[i], X3[i], 0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Green because the closest prediction would the 5, which equals green in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Red because it's the closest point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Large since large K value makes the decision boundary more linear in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
