{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Score Predictor\n",
    "\n",
    "This solution attempts to predict the scores of wines based on the description. This predictor uses TF-IDF (Term Frequency-Inverse Document Frequency) weighting method and KNN, both implemented in pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import sparse\n",
    "from matplotlib import pyplot as plt\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sparselsh import LSH\n",
    "import nltk\n",
    "# Uncomment on the first run\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and visualizing it a little bit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>None</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>None</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>None</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>None</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...   \n",
       "3        US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4        US  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                          designation  points  price           province  \\\n",
       "0                        Vulkà Bianco      87    NaN  Sicily & Sardinia   \n",
       "1                            Avidagos      87   15.0              Douro   \n",
       "2                                None      87   14.0             Oregon   \n",
       "3                Reserve Late Harvest      87   13.0           Michigan   \n",
       "4  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n",
       "\n",
       "              region_1           region_2         taster_name  \\\n",
       "0                 Etna               None       Kerin O’Keefe   \n",
       "1                 None               None          Roger Voss   \n",
       "2    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "3  Lake Michigan Shore               None  Alexander Peartree   \n",
       "4    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "\n",
       "  taster_twitter_handle                                              title  \\\n",
       "0          @kerinokeefe                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1            @vossroger      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2           @paulgwine       Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3                  None  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4           @paulgwine   Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "\n",
       "          variety               winery  \n",
       "0     White Blend              Nicosia  \n",
       "1  Portuguese Red  Quinta dos Avidagos  \n",
       "2      Pinot Gris            Rainstorm  \n",
       "3        Riesling           St. Julian  \n",
       "4      Pinot Noir         Sweet Cheeks  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_json('./data/winemag-data-130k-v2.json')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>129971.000000</td>\n",
       "      <td>120975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>88.447138</td>\n",
       "      <td>35.363389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.039730</td>\n",
       "      <td>41.022218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>91.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>3300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              points          price\n",
       "count  129971.000000  120975.000000\n",
       "mean       88.447138      35.363389\n",
       "std         3.039730      41.022218\n",
       "min        80.000000       4.000000\n",
       "25%        86.000000      17.000000\n",
       "50%        88.000000      25.000000\n",
       "75%        91.000000      42.000000\n",
       "max       100.000000    3300.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'designation',\n",
       " 'price',\n",
       " 'province',\n",
       " 'region_1',\n",
       " 'region_2',\n",
       " 'taster_name',\n",
       " 'taster_twitter_handle',\n",
       " 'variety']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_nan = [column for column in raw_data.columns \n",
    "                        if raw_data[column].isnull().any()]\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Description and points columns don't seem to contain any NaN's, so we can continue without worries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing required funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lemmatize(description)` function takes a list (here a description), and returns it in lemmatized form.\n",
    "\n",
    "For example, consider this list: \n",
    "```python \n",
    "['dogs', 'churches', 'is']\n",
    "```\n",
    "\n",
    "Function ```lemmatize(description)``` would return the previous list in the following form: \n",
    "```python \n",
    "['dog', 'church', 'is']\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(description):\n",
    "    '''Lemmatizes a given list and returns it'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for word in description:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "    return lemmatized    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```get_corpus(raw_descriptions``` and ```get_descriptions(raw_descriptions``` both are helper functions to help clean the data into nice lemmatized, and punctuationless form for further use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(raw_descriptions):\n",
    "    raw_descriptions_string = ' '.join(raw_descriptions).lower()\n",
    "    descriptions_string = ''.join([c if c not in punctuation else ' ' for c in raw_descriptions_string])\n",
    "    corpus = list(filter(lambda x: len(x) > 0, descriptions_string.split(' ')))\n",
    "    corpus = lemmatize(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(raw_descriptions):\n",
    "    '''Returns a list of descriptions that have been stripped of punctuation marks and lemmatized'''\n",
    "    descriptions = []\n",
    "    for d in raw_descriptions:\n",
    "        description_string = ''.join([c if c not in punctuation else ' ' for c in d]).lower()\n",
    "        description = list(filter(lambda x: len(x) > 0, description_string.split(' ')))\n",
    "        description = lemmatize(description)\n",
    "        descriptions.append(description)\n",
    "        \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```get_unique_words(corpus, X)``` returns a list of unique words, known as vocabulary in TF-IDF's context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(corpus):\n",
    "    '''Returns vocabulary, which is a list that only consists of unique words'''\n",
    "    vocabulary = []\n",
    "    for word in corpus:\n",
    "        if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```get_term_frequencies(train_X, vocabulary, vocabulary_indexes, rows, columns)``` returns the term frequencies in matrix form.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequencies(train_X, vocabulary, vocabulary_indexes, rows, columns):\n",
    "    '''Returns a matrix full of term frequencies'''\n",
    "    # Matrix that has as many rows as there are documents, and as many columns as there are words in the vocabulary\n",
    "    tfs = sparse.lil_matrix((rows, columns))\n",
    "    for description_index, description in enumerate(train_X):\n",
    "        for word in description:\n",
    "            if word in vocabulary:\n",
    "                word_index = vocabulary_indexes[word]\n",
    "                tfs[description_index, word_index] += 1\n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```get_inverse_document_frequencies(train_tfs, vocabulary_indexes, rows, columns)```  returns the inverse document frequencies in list form.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_document_frequencies(train_tfs, vocabulary_indexes, rows, columns):\n",
    "    '''Returns a list of inverse document frequencies'''\n",
    "    idfs = []\n",
    "    # N = total number of descriptions\n",
    "    N = columns\n",
    "    for column in range(columns):\n",
    "        #n_t = number of descriptions, that contain the term t\n",
    "        n_t = 0\n",
    "        for row in range(rows):\n",
    "            if train_tfs[row, column] > 0:\n",
    "                n_t += 1\n",
    "                \n",
    "        # Inverse Document Frequency = log(N/n_t)\n",
    "        idf = np.log(N/n_t)\n",
    "        idfs.append(idf)\n",
    "\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_descriptions = raw_data['description']\n",
    "descriptions_corpus = get_corpus(raw_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions = group of descriptions\n",
    "X = get_descriptions(raw_descriptions)[:10000]\n",
    "y = raw_data['points'][:10000]\n",
    "y = [int(point) for point in y]\n",
    "\n",
    "train_X, validation_X, train_y, validation_y = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vocabulary = unique words present in the train_X\n",
    "train_vocabulary = get_unique_words(reduce(lambda x, y: x + y, train_X, []))\n",
    "vocabulary_indexes = dict(map(lambda word: (word[1], word[0]), enumerate(train_vocabulary)))\n",
    "\n",
    "# n_dimensions = number of dimensions\n",
    "n_dimensions = len(train_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Since TF-IDF matrix contains so many zeroes, it would approximately require 47GB of ram to store it. This is why I decided to use sparse matrix instead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 10162)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf = Term Frequency, e.g. raw frequency of the term t in description d\n",
    "train_tfs = get_term_frequencies(train_X, train_vocabulary, vocabulary_indexes, len(train_X), n_dimensions)\n",
    "\n",
    "# idf = Inverse Document Frequency, e.g. weighted value based on the frequency in the whole corpus\n",
    "idfs = get_inverse_document_frequencies(train_tfs, vocabulary_indexes, len(train_X), n_dimensions)\n",
    "\n",
    "train_tf_idf = sparse.csr_matrix(train_tfs.multiply(np.array([idfs])))\n",
    "train_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3.0652032304586005\n",
      "  (0, 1)\t5.671062490664264\n",
      "  (0, 2)\t1.3087009185931995\n",
      "  (0, 3)\t4.328570752202766\n",
      "  (0, 4)\t1.5656893418262618\n",
      "  (0, 5)\t2.2755957837110934\n",
      "  (0, 6)\t4.382283905745161\n",
      "  (0, 7)\t2.3752256246599344\n",
      "  (0, 8)\t1.3967801630034844\n",
      "  (0, 9)\t2.8311489540382277\n",
      "  (0, 10)\t3.6060096864365274\n",
      "  (0, 11)\t0.9131853602021226\n",
      "  (0, 12)\t3.4395131707869697\n",
      "  (0, 13)\t4.011474794544692\n",
      "  (0, 14)\t1.4779505282539802\n",
      "  (0, 15)\t0.45240695195274583\n",
      "  (0, 16)\t1.471071739307176\n",
      "  (0, 17)\t1.6498007851806396\n",
      "  (0, 18)\t0.9482362612099396\n",
      "  (0, 19)\t2.2398440927472505\n",
      "  (0, 20)\t4.033453701263467\n",
      "  (0, 21)\t2.495392451671594\n",
      "  (0, 22)\t1.4507148022384317\n",
      "  (0, 23)\t1.5601886264909521\n",
      "  (0, 24)\t4.611290035312417\n",
      "  (0, 25)\t2.5443019547038683\n",
      "  (0, 26)\t4.249676809733103\n",
      "  (0, 27)\t4.235977965374941\n",
      "  (0, 28)\t5.294584919429352\n",
      "  (0, 29)\t4.170330859606373\n",
      "  (0, 30)\t3.280989943547102\n",
      "  (0, 31)\t3.559983864041245\n",
      "  (0, 32)\t1.2290837291555796\n",
      "  (0, 33)\t5.2374265055894025\n",
      "  (0, 34)\t0.5088921795039103\n",
      "  (0, 35)\t3.08008129448478\n",
      "  (0, 36)\t0.856326225775652\n",
      "  (0, 37)\t1.3770867341131166\n",
      "  (0, 38)\t2.9745066689877886\n",
      "  (0, 39)\t5.376262950443619\n",
      "  (0, 40)\t3.302154754739145\n",
      "  (0, 41)\t3.6169387569687177\n",
      "  (0, 42)\t7.2805004030983635\n",
      "  (0, 43)\t4.027913520887852\n",
      "  (0, 44)\t2.3720560498986556\n"
     ]
    }
   ],
   "source": [
    "print(train_tf_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tfs = get_term_frequencies(validation_X, train_vocabulary, vocabulary_indexes, len(validation_X), n_dimensions)\n",
    "\n",
    "validation_tf_idf = sparse.csr_matrix(validation_tfs.multiply(np.array([idfs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances between vectors\n",
    "\n",
    "I decided to use a locality-sensitive hashing engine to do the random binary projections, in order to store the vectors, and calculate distances between them efficiently. Later on, I've implemented brute force euclidean distance calculator in case the number of neighbors is less than k when querying the LSH engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_plane_count = 1\n",
    "hash_layers = 8\n",
    "engine = LSH(hyper_plane_count, train_tf_idf.shape[1], hash_layers)\n",
    "\n",
    "for vector_id in range(train_tf_idf.shape[0]):\n",
    "    vector = train_tf_idf.getrow(vector_id)\n",
    "    engine.index(vector, train_y[vector_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K_nearest_neighbours(vector, k):\n",
    "    '''Brute force approach to get k nearest neighbours'''\n",
    "    distances = []\n",
    "    for vector_id in range(validation_tf_idf.shape[0]):\n",
    "        current_vector = validation_tf_idf[vector_id]\n",
    "        label = train_y[vector_id]\n",
    "        sum_of_squares = 0\n",
    "        for i in range(validation_tf_idf.shape[1]):\n",
    "            sum_of_squares += np.square(vector[i] - current_vector[i])\n",
    "        distance = np.sqrt(sum_of_squares)\n",
    "        distances.append(([current_vector, label], distance))\n",
    "        \n",
    "        if len(distances) > k:\n",
    "            distances.sort(key = lambda x: x[1])\n",
    "            distances = distances[:k]\n",
    "    \n",
    "    distances.sort(key = lambda x: x[1])\n",
    "    \n",
    "    k_closest_neighbours = []\n",
    "    for i in range(k):\n",
    "        k_closest_neighbours.append(distances[i][0])\n",
    "\n",
    "    return k_closest_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X, k):\n",
    "    predictions = []\n",
    "    for vector in X:\n",
    "        neighbours = engine.query(vector, num_results = k)\n",
    "        neighbours.sort(key = lambda neighbour: neighbour[1])\n",
    "        nearest_points = list(map(lambda neighbour: neighbour[0][1], neighbours))\n",
    "        nearest_points = [int(point) for point in nearest_points]\n",
    "        if len(neighbours) < k:\n",
    "            neighbours = get_K_nearest_neighbours(vector, k)\n",
    "            nearest_points = list(map(lambda neighbour: neighbour[1], neighbours))\n",
    "\n",
    "        prediction = sum(nearest_points) / len(nearest_points)\n",
    "\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Since I don't have any test data (this dataset didn't include any competitions), validation data will have to do.\n",
    "\n",
    "-- Calculations haven't finished so far, I'll update this when they're done. --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = get_predictions(validation_tf_idf, 3)\n",
    "print(\"r2 score: \", r2_score(validation_y, predictions))\n",
    "\n",
    "plt.scatter(validation_y, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
