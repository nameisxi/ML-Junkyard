{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-layer Binary Classifier\n",
    "#### Implementation from scratch using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():    \n",
    "    def __init__(self, layers_dimensions):\n",
    "        self.layers_dimensions = layers_dimensions\n",
    "        self.parameters = None\n",
    "        \n",
    "        \n",
    "    def initialize_parameters(self, layer_dims):\n",
    "        parameters = {}\n",
    "        L = len(layer_dims)\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "            assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        cache = Z\n",
    "        return A, cache\n",
    "    \n",
    "    \n",
    "    def sigmoid_backpropagation(self, dA, cache):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single SIGMOID unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "        Z = cache\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def relu(self, Z):\n",
    "        A = np.maximum(0, Z)\n",
    "        cache = Z \n",
    "        assert(A.shape == Z.shape)\n",
    "        return A, cache\n",
    "    \n",
    "    \n",
    "    def relu_backpropagation(self, dA, cache):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)\n",
    "    \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "    \n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def forward_propagation_activation(self, A_prev, W, b, activation):\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "        assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        linear_cache = (A_prev, W, b)\n",
    "    \n",
    "        if activation == \"sigmoid\":\n",
    "            A, activation_cache = self.sigmoid(Z)\n",
    "        elif activation == \"relu\":\n",
    "            A, activation_cache = self.relu(Z)\n",
    "    \n",
    "        cache = (linear_cache, activation_cache)\n",
    "    \n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        return A, cache\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X, parameters):\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2               \n",
    "\n",
    "        for l in range(1, L):\n",
    "            A_prev = A \n",
    "            A, cache = self.forward_propagation_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "            caches.append(cache)\n",
    "            \n",
    "        AL, cache = self.forward_propagation_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid')\n",
    "        caches.append(cache)\n",
    "\n",
    "        assert(AL.shape == (1,X.shape[1]))     \n",
    "        return AL, caches\n",
    "    \n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL)))\n",
    "        cost = np.squeeze(cost) \n",
    "        \n",
    "        assert(cost.shape == ())\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backpropagation_activation(self, dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "        if activation == \"relu\":\n",
    "            dZ = self.relu_backpropagation(dA, activation_cache)\n",
    "            A_prev, W, b = linear_cache\n",
    "            m = A_prev.shape[1]\n",
    "            \n",
    "            dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "            db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = self.sigmoid_backpropagation(dA, activation_cache)\n",
    "            A_prev, W, b = linear_cache\n",
    "            m = A_prev.shape[1]\n",
    "            \n",
    "            dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "            db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (dW.shape == W.shape)\n",
    "        assert (db.shape == b.shape)\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        current_cache = caches[L - 1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.backpropagation_activation(dAL, current_cache, 'sigmoid')\n",
    "        \n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.backpropagation_activation(grads['dA' + str(l + 1)], current_cache, 'relu')\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def update_parameters(self, parameters, grads, learning_rate):\n",
    "        L = len(parameters) // 2\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l + 1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l + 1)]\n",
    "        return parameters\n",
    "       \n",
    "        \n",
    "    def plot_cost(self, costs, learning_rate):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()    \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, learning_rate=0.0075, num_iterations=3000, print_cost=False): #lr was 0.009\n",
    "        costs = []\n",
    "        parameters = self.initialize_parameters(self.layers_dimensions)\n",
    "        for i in range(num_iterations):\n",
    "            AL, caches = self.forward_propagation(X, parameters)\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "            grads = self.backpropagation(AL, Y, caches)\n",
    "            parameters = self.update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "                print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            \n",
    "        self.plot_cost(costs, learning_rate)\n",
    "        self.parameters = parameters\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        prediction = self.forward_propagation(X, self.parameters)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fraud_data = pd.read_csv('./data/fraud_dataset.csv')\n",
    "#fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for missing data imputation, since no data is missing at all.\n",
    "#fraud_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = np.array(fraud_data['Class'])\n",
    "#X = np.array(fraud_data.drop(columns=['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = Y.reshape(Y.shape[0], 1)\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, test_X, train_Y, test_Y = train_test_split(X, Y)\n",
    "#print(\"X_train shape:\", train_X.shape)\n",
    "#print(\"Y_train shape:\", train_Y.shape)\n",
    "#print(\"X_test shape:\", test_X.shape)\n",
    "#print(\"Y_test shape:\", test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_X = train_X.T\n",
    "#test_X = test_X.T\n",
    "#print(\"X_train shape:\", train_X.shape)\n",
    "#print(\"X_test shape:\", train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers_dimensions = [train_X.shape[0], 20, 7, 5, 1] \n",
    "#model = NN(layers_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.train(train_X, train_Y, learning_rate=0.001, num_iterations=5000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
